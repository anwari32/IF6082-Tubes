{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e22894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b749515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utils\n",
    "def max(a, b):\n",
    "    return a if a > b else b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d534ad5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_html</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>lang</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is a novel coronavirus?</td>\n",
       "      <td>A novel coronavirus is a new coronavirus that ...</td>\n",
       "      <td>&lt;p&gt;A novel coronavirus is a new coronavirus th...</td>\n",
       "      <td>\\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...</td>\n",
       "      <td>Frequently Asked Questions</td>\n",
       "      <td>Center for Disease Control and Prevention (CDC)</td>\n",
       "      <td>Coronavirus Disease 2019 Basics</td>\n",
       "      <td>USA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td>2020/03/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is the disease being called coronavirus di...</td>\n",
       "      <td>On February 11, 2020 the World Health Organiza...</td>\n",
       "      <td>&lt;p&gt;On February 11, 2020 the World Health Organ...</td>\n",
       "      <td>\\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...</td>\n",
       "      <td>Frequently Asked Questions</td>\n",
       "      <td>Center for Disease Control and Prevention (CDC)</td>\n",
       "      <td>Coronavirus Disease 2019 Basics</td>\n",
       "      <td>USA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td>2020/03/17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                       What is a novel coronavirus?   \n",
       "1  Why is the disease being called coronavirus di...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  A novel coronavirus is a new coronavirus that ...   \n",
       "1  On February 11, 2020 the World Health Organiza...   \n",
       "\n",
       "                                         answer_html  \\\n",
       "0  <p>A novel coronavirus is a new coronavirus th...   \n",
       "1  <p>On February 11, 2020 the World Health Organ...   \n",
       "\n",
       "                                                link  \\\n",
       "0  \\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...   \n",
       "1  \\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...   \n",
       "\n",
       "                         name  \\\n",
       "0  Frequently Asked Questions   \n",
       "1  Frequently Asked Questions   \n",
       "\n",
       "                                            source  \\\n",
       "0  Center for Disease Control and Prevention (CDC)   \n",
       "1  Center for Disease Control and Prevention (CDC)   \n",
       "\n",
       "                          category country region city lang last_update  \n",
       "0  Coronavirus Disease 2019 Basics     USA               en  2020/03/17  \n",
       "1  Coronavirus Disease 2019 Basics     USA               en  2020/03/17  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reads CSVs\n",
    "def read_csv(file, encoding):\n",
    "    df = pd.read_csv(file, encoding=encoding)\n",
    "    df.fillna(value=\"\", inplace=True)\n",
    "    return df\n",
    "\n",
    "# Read covid BERT documents\n",
    "faq_covidbert = read_csv(\"data/faqs/faq_covidbert.csv\", \"utf8\")\n",
    "faq_covidbert.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf737665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/wizard/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "# Preprocess the text with POS Tagger\n",
    "# @input - sentence : string\n",
    "# @return - tuples : array of tuples.\n",
    "def pos_tagger(sentence):\n",
    "    pretrainTagger = PerceptronTagger()\n",
    "    tuples = pretrainTagger.tag(sentence.split())\n",
    "    return tuples\n",
    "\n",
    "# Test tagger for a sentence.\n",
    "tagged_answer0 = pos_tagger(faq_covidbert.iloc[0]['answer'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dff3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_answers = [pos_tagger(s) for s in faq_covidbert['answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5d2939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "import numpy as np\n",
    "\n",
    "GLOVE_PATH = 'glove/glove.6B.200d.txt'\n",
    "def load_glove(glove_path):\n",
    "    f = open(glove_path,encoding=\"utf8\")\n",
    "    embeddings_index = dict()\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embeddings = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embeddings\n",
    "    f.close()\n",
    "    print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings = load_glove(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d424a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wizard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello', 'world', ',', 'dude']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [str.lower(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize_sentence(\"Hello World, Dude\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c57d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 entry(s) of knowledge base.\n",
      "2019 question(s)\n",
      "max possible answer for a question is 1\n",
      "max word length in sentence is 157\n"
     ]
    }
   ],
   "source": [
    "# JSON data processor\n",
    "# Read JSON file and isolate the knowledge base, questions, and answers.\n",
    "# @input - json file path : string\n",
    "# @output - list of questions and its corresponding answers.\n",
    "import json\n",
    "def read_json(file_path):\n",
    "    f = open(file_path, encoding='utf8')\n",
    "    json_data = json.load(f)\n",
    "    data = json_data['data']\n",
    "    max_possible_answer = 0\n",
    "    max_word_length = 0\n",
    "    qa_tuples = []\n",
    "    for datum in data:\n",
    "        paragraphs = datum['paragraphs']\n",
    "        for paragraph in paragraphs:\n",
    "            qas = paragraph['qas']\n",
    "            for qa in qas:\n",
    "                question = qa['question']\n",
    "                answers = qa['answers']\n",
    "                qa_tuples.append({'question' : question, 'answers': answers})\n",
    "                max_possible_answer = max(max_possible_answer, len(answers))\n",
    "                for answer in answers:\n",
    "                    a = answer\n",
    "                    a_tokens = word_tokenize(a['text'])\n",
    "                    max_word_length = max(max_word_length, len(a_tokens))\n",
    "                    \n",
    "    print('{} entry(s) of knowledge base.'.format(len(data)))\n",
    "    print('{} question(s)'.format(len(qa_tuples)))\n",
    "    print('max possible answer for a question is {}'.format(max_possible_answer))\n",
    "    print('max word length in sentence is {}'.format(max_word_length))\n",
    "    return data, qa_tuples\n",
    "\n",
    "faq_data, qa_tuples = read_json('data/question-answering/COVID-QA.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d69cb612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.39396   ,  0.44185001, -0.0042279 , ...,  0.47576001,\n",
       "         0.20977999, -0.11687   ],\n",
       "       [ 0.036749  ,  0.19893999, -0.093035  , ..., -0.013302  ,\n",
       "        -0.0039236 ,  0.71275997],\n",
       "       [ 0.85395002,  0.57146001, -0.023652  , ...,  0.31083   ,\n",
       "        -0.22303   ,  0.20370001],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 200\n",
    "DIMENSION_SIZE = 200\n",
    "\n",
    "# Create vector representation of question.\n",
    "# @input - question : string\n",
    "# @output - vector representation of question.\n",
    "def create_embedding_matrix(question, embeddings):\n",
    "    if(not embeddings):\n",
    "        print('please provide embeddings')\n",
    "        quit()\n",
    "    if(not question):\n",
    "        print('please provide question')\n",
    "        quit()\n",
    "    embedding_matrix = np.zeros((VOCAB_SIZE, DIMENSION_SIZE))\n",
    "    tokens = tokenize_sentence(question)\n",
    "    for token in tokens:\n",
    "        embedding_vector = embeddings.get(token)\n",
    "        embedding_matrix[tokens.index(token)] = embedding_vector\n",
    "        # print('{} => {}'.format(token, embedding_vector))\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "question = \"What are you doing?\"\n",
    "embeddings = load_glove(GLOVE_PATH)\n",
    "embedding_matrix = create_embedding_matrix(question, embeddings)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26f4e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "# @input - a, b : matrix\n",
    "# @output - similarity score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def calculate_cosine_similarity(a, b):\n",
    "    score = cosine_similarity(a, b)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b916f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentence similarity between a matrix and a set of matrices\n",
    "# @input - a : matrix, bs : set of matrix\n",
    "# @output - index of which matrix having best similarity score\n",
    "def calculate_sentence_similarity(a, bs):\n",
    "    scores = [calculate_consine_similarity(a, b) for b in bs]\n",
    "    max_score = max(scores)\n",
    "    max_score_index = scores.index(max_score)\n",
    "    return max_score_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af1a4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for qa in qa_tuples:\n",
    "    X_train.append(create_embedding_matrix(qa['question'], embeddings))\n",
    "    Y_train.append(create_embedding_matrix(qa['answers'][0]['text'], embeddings))\n",
    "    \n",
    "training_fraction = int(0.8 * len(X_train))\n",
    "X_train = X_train[:training_fraction]\n",
    "Y_train = Y_train[:training_fraction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "665c6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data\n",
    "X_validation = X_train[training_fraction:]\n",
    "Y_validation = Y_train[training_fraction:]\n",
    "\n",
    "test_fraction = int(0.5 * len(X_validation))\n",
    "X_validation = X_validation[:test_fraction]\n",
    "Y_validation = Y_validation[:test_fraction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "062aec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "X_test = X_validation[:test_fraction]\n",
    "Y_test = Y_validation[:test_fraction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40489f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         40000     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 200)         200200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 128)         25728     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 200)         128200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 128)         25728     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 552,730\n",
      "Trainable params: 552,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating model.\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "INPUT_DIMENSION = 200 # using GloVe Embedding 100 dimensions.\n",
    "OUTPUT_DIMENSION = 200 # output embedding layer 50 dimensions.\n",
    "KERNEL_SIZE = 5 # Kernel size.\n",
    "\n",
    "# Model architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=INPUT_DIMENSION, output_dim=OUTPUT_DIMENSION))\n",
    "model.add(layers.Convolution1D(filters=INPUT_DIMENSION, kernel_size=KERNEL_SIZE))\n",
    "model.add(layers.MaxPooling1D(pool_size=2, strides=None, padding=\"valid\"))\n",
    "model.add(layers.Dense(128))\n",
    "model.add(layers.Convolution1D(filters=INPUT_DIMENSION, kernel_size=KERNEL_SIZE))\n",
    "model.add(layers.MaxPooling1D(pool_size=2, strides=None, padding=\"valid\"))\n",
    "model.add(layers.Dense(128))\n",
    "model.add(layers.LSTM(128))\n",
    "model.add(layers.Dense(10))\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b27cf60b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.callbacks.ModelCheckpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ee6db0b37b4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m callback = Checkpoint(filepath='models/12042021', \n\u001b[1;32m      5\u001b[0m                       \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.callbacks.ModelCheckpoint'"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "# Callback\n",
    "import tensorflow.keras.callbacks\n",
    "callback = Checkpoint(filepath='models/12042021', \n",
    "                      monitor='val_accuracy',\n",
    "                      mode='max',\n",
    "                      save_best_only=True)\n",
    "model.fit(X_training, Y_training, batch_size=10, epochs=10, validation_data=(X_validation, Y_validation), verbose=True, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4713c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6eff0678",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAlbertTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-9e1d8cd818e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'albert-base-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'albert-base-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/transformers/utils/dummy_sentencepiece_objects.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mrequires_sentencepiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mrequires_sentencepiece\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__name__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sentencepiece_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTENCEPIECE_IMPORT_ERROR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nAlbertTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment.\n"
     ]
    }
   ],
   "source": [
    "# Exploration with ALBERT\n",
    "from transformers import AlbertTokenizer, AlbertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertForQuestionAnswering.from_pretrained('albert-base-v2')\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "inputs = tokenizer(question, text, return_tensors='pt')\n",
    "start_positions = torch.tensor([1])\n",
    "end_positions = torch.tensor([3])\n",
    "\n",
    "outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "loss = outputs.loss\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26325dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d173bb60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
