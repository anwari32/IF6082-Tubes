{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e22894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea42f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utils\n",
    "def max(a, b):\n",
    "    return a if a > b else b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d534ad5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_html</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>lang</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is a novel coronavirus?</td>\n",
       "      <td>A novel coronavirus is a new coronavirus that ...</td>\n",
       "      <td>&lt;p&gt;A novel coronavirus is a new coronavirus th...</td>\n",
       "      <td>\\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...</td>\n",
       "      <td>Frequently Asked Questions</td>\n",
       "      <td>Center for Disease Control and Prevention (CDC)</td>\n",
       "      <td>Coronavirus Disease 2019 Basics</td>\n",
       "      <td>USA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td>2020/03/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is the disease being called coronavirus di...</td>\n",
       "      <td>On February 11, 2020 the World Health Organiza...</td>\n",
       "      <td>&lt;p&gt;On February 11, 2020 the World Health Organ...</td>\n",
       "      <td>\\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...</td>\n",
       "      <td>Frequently Asked Questions</td>\n",
       "      <td>Center for Disease Control and Prevention (CDC)</td>\n",
       "      <td>Coronavirus Disease 2019 Basics</td>\n",
       "      <td>USA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>en</td>\n",
       "      <td>2020/03/17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                       What is a novel coronavirus?   \n",
       "1  Why is the disease being called coronavirus di...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  A novel coronavirus is a new coronavirus that ...   \n",
       "1  On February 11, 2020 the World Health Organiza...   \n",
       "\n",
       "                                         answer_html  \\\n",
       "0  <p>A novel coronavirus is a new coronavirus th...   \n",
       "1  <p>On February 11, 2020 the World Health Organ...   \n",
       "\n",
       "                                                link  \\\n",
       "0  \\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...   \n",
       "1  \\nhttps://www.cdc.gov/coronavirus/2019-ncov/fa...   \n",
       "\n",
       "                         name  \\\n",
       "0  Frequently Asked Questions   \n",
       "1  Frequently Asked Questions   \n",
       "\n",
       "                                            source  \\\n",
       "0  Center for Disease Control and Prevention (CDC)   \n",
       "1  Center for Disease Control and Prevention (CDC)   \n",
       "\n",
       "                          category country region city lang last_update  \n",
       "0  Coronavirus Disease 2019 Basics     USA               en  2020/03/17  \n",
       "1  Coronavirus Disease 2019 Basics     USA               en  2020/03/17  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reads CSVs\n",
    "def read_csv(file, encoding):\n",
    "    df = pd.read_csv(file, encoding=encoding)\n",
    "    df.fillna(value=\"\", inplace=True)\n",
    "    return df\n",
    "\n",
    "# Read covid BERT documents\n",
    "faq_covidbert = read_csv(\"data/faqs/faq_covidbert.csv\", \"utf8\")\n",
    "faq_covidbert.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf737665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/wizard/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "# Preprocess the text with POS Tagger\n",
    "# @input - sentence : string\n",
    "# @return - tuples : array of tuples.\n",
    "def pos_tagger(sentence):\n",
    "    pretrainTagger = PerceptronTagger()\n",
    "    tuples = pretrainTagger.tag(sentence.split())\n",
    "    return tuples\n",
    "\n",
    "# Test tagger for a sentence.\n",
    "tagged_answer0 = pos_tagger(faq_covidbert.iloc[0]['answer'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dff3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_answers = [pos_tagger(s) for s in faq_covidbert['answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5d2939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "import numpy as np\n",
    "\n",
    "GLOVE_PATH = 'glove/glove.6B.200d.txt'\n",
    "def load_glove(glove_path):\n",
    "    f = open(glove_path,encoding=\"utf8\")\n",
    "    embeddings_index = dict()\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embeddings = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embeddings\n",
    "    f.close()\n",
    "    print('Found {} word vectors.'.format(len(embeddings_index)))\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings = load_glove(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf240ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wizard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello', 'world', ',', 'dude']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [str.lower(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize_sentence(\"Hello World, Dude\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c57d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 entry(s) of knowledge base.\n",
      "2019 question(s)\n",
      "max possible answer for a question is 1\n",
      "max word length in sentence is 157\n"
     ]
    }
   ],
   "source": [
    "# JSON data processor\n",
    "# Read JSON file and isolate the knowledge base, questions, and answers.\n",
    "# @input - json file path : string\n",
    "# @output - list of questions and its corresponding answers.\n",
    "import json\n",
    "def read_json(file_path):\n",
    "    f = open(file_path, encoding='utf8')\n",
    "    json_data = json.load(f)\n",
    "    data = json_data['data']\n",
    "    max_possible_answer = 0\n",
    "    max_word_length = 0\n",
    "    qa_tuples = []\n",
    "    for datum in data:\n",
    "        paragraphs = datum['paragraphs']\n",
    "        for paragraph in paragraphs:\n",
    "            qas = paragraph['qas']\n",
    "            for qa in qas:\n",
    "                question = qa['question']\n",
    "                answers = qa['answers']\n",
    "                qa_tuples.append({'question' : question, 'answers': answers})\n",
    "                max_possible_answer = max(max_possible_answer, len(answers))\n",
    "                for answer in answers:\n",
    "                    a = answer\n",
    "                    a_tokens = word_tokenize(a['text'])\n",
    "                    max_word_length = max(max_word_length, len(a_tokens))\n",
    "                    \n",
    "    print('{} entry(s) of knowledge base.'.format(len(data)))\n",
    "    print('{} question(s)'.format(len(qa_tuples)))\n",
    "    print('max possible answer for a question is {}'.format(max_possible_answer))\n",
    "    print('max word length in sentence is {}'.format(max_word_length))\n",
    "    return data, qa_tuples\n",
    "\n",
    "faq_data, qa_tuples = read_json('data/question-answering/COVID-QA.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff7d8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.39396   ,  0.44185001, -0.0042279 , ...,  0.47576001,\n",
       "         0.20977999, -0.11687   ],\n",
       "       [ 0.036749  ,  0.19893999, -0.093035  , ..., -0.013302  ,\n",
       "        -0.0039236 ,  0.71275997],\n",
       "       [ 0.85395002,  0.57146001, -0.023652  , ...,  0.31083   ,\n",
       "        -0.22303   ,  0.20370001],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 200\n",
    "DIMENSION_SIZE = 200\n",
    "\n",
    "# Create vector representation of question.\n",
    "# @input - question : string\n",
    "# @output - vector representation of question.\n",
    "def create_embedding_matrix(question, embeddings):\n",
    "    if(not embeddings):\n",
    "        print('please provide embeddings')\n",
    "        quit()\n",
    "    if(not question):\n",
    "        print('please provide question')\n",
    "        quit()\n",
    "    embedding_matrix = np.zeros((VOCAB_SIZE, DIMENSION_SIZE))\n",
    "    tokens = tokenize_sentence(question)\n",
    "    for token in tokens:\n",
    "        embedding_vector = embeddings.get(token)\n",
    "        embedding_matrix[tokens.index(token)] = embedding_vector\n",
    "        # print('{} => {}'.format(token, embedding_vector))\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "question = \"What are you doing?\"\n",
    "embeddings = load_glove(GLOVE_PATH)\n",
    "embedding_matrix = create_embedding_matrix(question, embeddings)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63b54cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "# @input - a, b : matrix\n",
    "# @output - similarity score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def calculate_cosine_similarity(a, b):\n",
    "    score = cosine_similarity(a, b)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e87eb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentence similarity between a matrix and a set of matrices\n",
    "# @input - a : matrix, bs : set of matrix\n",
    "# @output - index of which matrix having best similarity score\n",
    "def calculate_sentence_similarity(a, bs):\n",
    "    scores = [calculate_consine_similarity(a, b) for b in bs]\n",
    "    max_score = max(scores)\n",
    "    max_score_index = scores.index(max_score)\n",
    "    return max_score_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "000a6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for qa in qa_tuples:\n",
    "    X_train.append(create_embedding_matrix(qa['question'], embeddings))\n",
    "    Y_train.append(create_embedding_matrix(qa['answers'][0]['text'], embeddings))\n",
    "    \n",
    "training_fraction = int(0.8 * len(X_train))\n",
    "X_train = X_train[:training_fraction]\n",
    "Y_train = Y_train[:training_fraction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d2be4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data\n",
    "X_validation = X_train[training_fraction:]\n",
    "Y_validation = Y_train[training_fraction:]\n",
    "\n",
    "test_fraction = int(0.5 * len(X_validation))\n",
    "X_validation = X_validation[:test_fraction]\n",
    "Y_validation = Y_validation[:test_fraction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1c35bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "X_test = X_validation[:test_fraction]\n",
    "Y_test = Y_validation[:test_fraction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40489f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 200)         40000     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 200)         200200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 128)         25728     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 200)         128200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 128)         25728     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 552,730\n",
      "Trainable params: 552,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating model.\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "INPUT_DIMENSION = 200 # using GloVe Embedding 200 dimensions.\n",
    "OUTPUT_DIMENSION = 200 # output embedding layer 50 dimensions.\n",
    "KERNEL_SIZE = 5 # Kernel size.\n",
    "\n",
    "# Model architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=INPUT_DIMENSION, output_dim=OUTPUT_DIMENSION))\n",
    "model.add(layers.Convolution1D(filters=INPUT_DIMENSION, kernel_size=KERNEL_SIZE))\n",
    "model.add(layers.MaxPooling1D(pool_size=2, strides=None, padding=\"valid\"))\n",
    "model.add(layers.Dense(128))\n",
    "model.add(layers.Convolution1D(filters=INPUT_DIMENSION, kernel_size=KERNEL_SIZE))\n",
    "model.add(layers.MaxPooling1D(pool_size=2, strides=None, padding=\"valid\"))\n",
    "model.add(layers.Dense(128))\n",
    "model.add(layers.LSTM(128))\n",
    "model.add(layers.Dense(10))\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "289b0789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:1569 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4941 sparse_categorical_crossentropy\n        labels=target, logits=output)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py:4241 sparse_softmax_cross_entropy_with_logits_v2\n        labels=labels, logits=logits, name=name)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py:4156 sparse_softmax_cross_entropy_with_logits\n        logits.get_shape()))\n\n    ValueError: Shape mismatch: The shape of labels (received (2000,)) should equal the shape of logits except for the last dimension (received (10, 10)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3bb43d89765d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                            \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                            save_best_only=True)\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:756 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:1569 sparse_categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4941 sparse_categorical_crossentropy\n        labels=target, logits=output)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py:4241 sparse_softmax_cross_entropy_with_logits_v2\n        labels=labels, logits=logits, name=name)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /home/wizard/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py:4156 sparse_softmax_cross_entropy_with_logits\n        logits.get_shape()))\n\n    ValueError: Shape mismatch: The shape of labels (received (2000,)) should equal the shape of logits except for the last dimension (received (10, 10)).\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "# Callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "callback = ModelCheckpoint(filepath='models/12042021', \n",
    "                           monitor='val_accuracy', \n",
    "                           mode='max',\n",
    "                           save_best_only=True)\n",
    "model.fit(X_train, Y_train, batch_size=10, epochs=10, validation_data=(X_validation, Y_validation), verbose=True, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e8b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6eff0678",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAlbertTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-9e1d8cd818e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'albert-base-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlbertForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'albert-base-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/transformers/utils/dummy_sentencepiece_objects.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mrequires_sentencepiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Reference/lectures/IF6082 NLP/virtualenv/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mrequires_sentencepiece\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__name__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sentencepiece_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTENCEPIECE_IMPORT_ERROR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nAlbertTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment.\n"
     ]
    }
   ],
   "source": [
    "# Exploration with ALBERT\n",
    "from transformers import AlbertTokenizer, AlbertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertForQuestionAnswering.from_pretrained('albert-base-v2')\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "inputs = tokenizer(question, text, return_tensors='pt')\n",
    "start_positions = torch.tensor([1])\n",
    "end_positions = torch.tensor([3])\n",
    "\n",
    "outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "loss = outputs.loss\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5cdaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d173bb60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
